{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Noisy Student Training Implementation\n",
    "\n",
    "This is an implementation of the [\"Noisy Student\"](https://arxiv.org/abs/1911.04252) article, in which a teacher model is trained on a small quantity of labeled data, and is then used to produce pseudolabels for a much larger quantity of unlabeled data. The pseduolabeled data is then perturbed, and used to train a student model, which in turn produces new labels for the data. This process of repeated learning, labeling, and re-learning of perturbed data can be repeated to increase the overall accuracy of the model and improve its performance with new data. I have used the [Painter by Numbers](https://www.kaggle.com/competitions/painter-by-numbers/data) dataset from Kaggle in this implementation."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "\n",
    "from helpers import ImageDataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Set the locations of the training dataset and unlabeled dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "labeled_image_root = r''\n",
    "labeled_annotations = r'' #A csv containing rows with [index, filename, encoded label]\n",
    "unlabeled_image_root = r''\n",
    "unlabeled_annotations = r'' #A csv containing rows with [index, filename]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "labeled_df = pd.read_csv(labeled_annotations, names=['index', 'filename', 'style'], header=None)\n",
    "unlabeled_df = pd.read_csv(unlabeled_annotations, names=['index', 'filename', 'style'], header=None)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Set the save path for the models\n",
    "save_folder = r''"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Set up the transform to ensure the images are in the size the model expects and normalise them."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Resize((32, 32)),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Set the batch size"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_size = 4"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load in the training and validation datasets with the labeled data, ensuring it is transformed correctly"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "labeled_train_dataset = ImageDataset(\n",
    "    root_dir=labeled_image_root,\n",
    "    annotations=labeled_df,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "labeled_train_dataset, labeled_test_dataset = train_test_split(labeled_train_dataset, test_size=0.2) # We take 20% of the data for a final test set\n",
    "labeled_train_dataset, labeled_validation_dataset = train_test_split(labeled_train_dataset, test_size=0.25) # We take 20% of the original data for a validation set\n",
    "\n",
    "labeled_train_data = DataLoader(dataset=labeled_train_dataset, shuffle=True, batch_size=batch_size)\n",
    "labeled_validation_data = DataLoader(dataset=labeled_validation_dataset, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "labeled_test_data = DataLoader(dataset=labeled_test_dataset, shuffle=True, batch_size=batch_size)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Import the model, training function, set the training parameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from models import CNN\n",
    "from train import train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Visualisation of a piece of data going into the model to check everything looks correct."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "train_features, train_labels, image_id = next(iter(labeled_train_data))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "print(train_features[0].shape)\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "image_id = image_id[0]\n",
    "print(f\"Label: {label}\")\n",
    "print(f'Image ID: {image_id}')\n",
    "plt.imshow(img.T)\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "lr = 0.001\n",
    "momentum = 0.9\n",
    "epochs = 2\n",
    "criterion = nn.CrossEntropyLoss()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train(model=CNN(), training_data=labeled_train_data, validation_data=labeled_validation_data, device=device, criterion=criterion, lr=lr, momentum=momentum, epochs=epochs, save=False, save_path=save_folder)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Using the teacher to produce pseudo-labeled data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "unlabeled_dataset = ImageDataset(\n",
    "    root_dir=unlabeled_image_root,\n",
    "    annotations=unlabeled_df,\n",
    "    transform=transform,\n",
    "    labels=False\n",
    ")\n",
    "\n",
    "unlabeled_data = DataLoader(dataset=unlabeled_dataset, shuffle=True, batch_size=batch_size)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Check that we're inputting the images correctly"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "unlabl_features, unlabl_image_id = next(iter(unlabeled_data))\n",
    "print(f\"Feature batch shape: {unlabl_features.size()}\")\n",
    "#print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "print(unlabl_features[0].shape)\n",
    "img_u = unlabl_features[0].squeeze()\n",
    "#label = train_labels[0]\n",
    "#print(f\"Label: {label}\")\n",
    "plt.imshow(img_u.T)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use a normal (i.e., not noised) teacher model to generate soft or hard pseudo labels for clean (i.e., not distorted) unlabeled images"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = CNN()\n",
    "model.eval()\n",
    "\n",
    "pseudolabel_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(unlabeled_data, 0):\n",
    "        outputs = model(data[0])\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        for j in list(zip(data[1], predicted.tolist())):\n",
    "            pseudolabel_list.append(j)\n",
    "\n",
    "        #print(f'i: {data[1]}, Predicted: {predicted}')\n",
    "\n",
    "\n",
    "pseudolabel_df = pd.DataFrame(pseudolabel_list, columns=['filename', 'style'])\n",
    "pseudolabel_df.insert(loc=0, column='index', value=pseudolabel_df.index)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pseudolabel_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "combined_df = pd.concat([labeled_df, pseudolabel_df], ignore_index=True)\n",
    "combined_df['index'] = combined_df.index"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "combined_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train a student model which minimizes the cross entropy loss on a combination of labeled and pseudo-labeled images with noise added to the student model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "noisy_transform = transforms.Compose(\n",
    "    [transforms.TrivialAugmentWide(),\n",
    "     transforms.Resize((32, 32)),\n",
    "     transforms.ToTensor()\n",
    "     ])\n",
    "\n",
    "hardlabel_train_dataset = ImageDataset(\n",
    "    root_dir=labeled_image_root,\n",
    "    annotations=combined_df,\n",
    "    transform=noisy_transform\n",
    ")\n",
    "\n",
    "hardlabel_train_dataset, hardlabel_valid_dataset = train_test_split(hardlabel_train_dataset, test_size=0.2) # We take 20% of the data for a validation test set\n",
    "\n",
    "hardlabel_train_data = DataLoader(dataset=hardlabel_train_dataset, shuffle=True, batch_size=batch_size)\n",
    "hardlabel_valid_data = DataLoader(dataset=hardlabel_valid_dataset, shuffle=True, batch_size=batch_size)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "train_features, train_labels, image_id = next(iter(hardlabel_train_data))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "print(train_features[0].shape)\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "image_id = image_id[0]\n",
    "print(img.dtype)\n",
    "print(f\"Label: {label}\")\n",
    "print(f'Image ID: {image_id}')\n",
    "plt.imshow(img.T)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train(model=CNN(), training_data=hardlabel_train_data, validation_data=hardlabel_valid_data, device=device, criterion=criterion, lr=lr, momentum=momentum, epochs=epochs, save=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "pseudolabel_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(unlabeled_data, 0):\n",
    "        outputs = model(data[0])\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        for j in list(zip(data[1], predicted.tolist())):\n",
    "            pseudolabel_list.append(j)\n",
    "\n",
    "combined_df = pd.DataFrame(pseudolabel_list, columns=['filename', 'style'])\n",
    "combined_df.insert(loc=0, column='index', value=pseudolabel_df.index)\n",
    "\n",
    "combined_df[combined_df.iloc[:, 1].isin(labeled_df.iloc[:, 1])] = labeled_df\n",
    "combined_df['index'] = combined_df.index"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
